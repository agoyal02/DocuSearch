{
  "title": "How Context Affects Language Models' Factual Predictions",
  "author": [
    {
      "first_name": "Fabio",
      "last_name": "Petroni",
      "full_name": "Fabio Petroni"
    },
    {
      "first_name": "Patrick",
      "last_name": "Lewis",
      "full_name": "Patrick Lewis"
    },
    {
      "first_name": "Tim",
      "last_name": "Rocktäschel",
      "full_name": "Tim Rocktäschel"
    },
    {
      "first_name": "Yuxiang",
      "last_name": "Wu",
      "full_name": "Yuxiang Wu"
    },
    {
      "first_name": "Alexander",
      "last_name": "Miller",
      "full_name": "Alexander Miller"
    },
    {
      "first_name": "Sebastian",
      "last_name": "Riedel",
      "full_name": "Sebastian Riedel"
    }
  ],
  "abstract": "When pre-trained on large unsupervised textual corpora, language models are able to store and retrieve factual knowledge to some extent, making it possible to use them directly for zero-shot cloze-style question answering. However, storing factual knowledge in a fixed number of weights of a language model clearly has limitations. Previous approaches have successfully provided access to information outside the model weights using supervised architectures that combine an information retrieval system with a machine reading component. In this paper, we go a step further and integrate information from a retrieval system with a pre-trained language model in a purely unsupervised way. We report that augmenting pre-trained language models in this way dramatically improves performance and that the resulting system, despite being unsupervised, is competitive with a supervised machine reading baseline. Furthermore, processing query and context with different segment tokens allows BERT to utilize its Next Sentence Prediction pre-trained classifier to determine whether the context is relevant or not, substantially improving BERT's zeroshot cloze-style question-answering performance and making its predictions robust to noisy contexts.",
  "published_date": "10 May 2020",
  "topic": "",
  "text": "IntroductionPre-trained language models such as BERT [Devlin et al., 2019] and RoBERTa [Liu et al., 2019] enabled state-of-the-art in many downstream NLP tasks [Wang et al., 2018a, 2019, Wu et al., 2019]. These models are trained in an unsupervised way from large textual collection and recent work [Petroni et al., 2019, Jiang et al., 2019, Talmor et al., 2019, Devlin et al., 2019] has demonstrated that such language models can store factual knowledge to some extent. However, considering the mill...",
  "file_type": "PDF",
  "upload_date": "2025-09-16T21:12:57.400687",
  "job_id": "88dbba37",
  "parser": "GROBID",
  "file_size": 599811
}