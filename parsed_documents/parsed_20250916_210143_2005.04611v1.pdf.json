{
  "title": "How Context Affects Language Models' Factual Predictions",
  "author": [
    {
      "first_name": "Fabio",
      "last_name": "Petroni",
      "full_name": "Fabio Petroni"
    },
    {
      "first_name": "Patrick",
      "last_name": "Lewis",
      "full_name": "Patrick Lewis"
    },
    {
      "first_name": "Tim",
      "last_name": "Rocktäschel",
      "full_name": "Tim Rocktäschel"
    },
    {
      "first_name": "Yuxiang",
      "last_name": "Wu",
      "full_name": "Yuxiang Wu"
    },
    {
      "first_name": "Alexander",
      "last_name": "Miller",
      "full_name": "Alexander Miller"
    },
    {
      "first_name": "Sebastian",
      "last_name": "Riedel",
      "full_name": "Sebastian Riedel"
    }
  ],
  "published_date": "10 May 2020",
  "topic": "",
  "text": "IntroductionPre-trained language models such as BERT [Devlin et al., 2019] and RoBERTa [Liu et al., 2019] enabled state-of-the-art in many downstream NLP tasks [Wang et al., 2018a, 2019, Wu et al., 2019]. These models are trained in an unsupervised way from large textual collection and recent work [Petroni et al., 2019, Jiang et al., 2019, Talmor et al., 2019, Devlin et al., 2019] has demonstrated that such language models can store factual knowledge to some extent. However, considering the mill...",
  "file_type": "PDF",
  "upload_date": "2025-09-16T21:01:47.166975",
  "job_id": "3dff0e24",
  "parser": "GROBID",
  "file_size": 599811
}